{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning for Sentiment Analysis\n",
    "\n",
    "This notebook aims to provide an example of how a Recurrent Neural Network (RNN) using the Long Short Term Memory (LSTM) architecture can be implemented using Keras(https://keras.io/). Keras is a high-level neural networks API, written in Python and capable of running on top of TensorFlow, CNTK, or Theano. It was developed with a focus on enabling fast experimentation. \n",
    "\n",
    "In this notebook, we have experimented with LSTM to perform sentiment analysis on movie reviews from the Large Movie Review Dataset(http://ai.stanford.edu/~amaas/data/sentiment/), better known as the IMDB dataset.\n",
    "\n",
    "In this task, given a movie review, the model attempts to predict whether it is positive or negative. This is a binary classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/chrisliu/.local/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# from keras.datasets import imdb\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Activation, Embedding, Bidirectional, LSTM, Input, merge, SpatialDropout1D, Lambda, Layer\n",
    "from keras.layers.convolutional import Conv1D, MaxPooling1D\n",
    "from keras.layers.normalization  import BatchNormalization\n",
    "# from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "from keras import backend as K\n",
    "from keras.initializers import Constant\n",
    "from nltk.tokenize import word_tokenize\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re\n",
    "import copy\n",
    "import time\n",
    "import random\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "As previously mentioned, we shall train a LSTM recurrent neural network on the Large Movie Review Dataset dataset.\n",
    "\n",
    "The Large Movie Review Dataset (often referred to as the IMDB dataset) contains 25,000 highly-polar movie reviews (good or bad) for training and the same amount again for testing. The problem is to determine whether a given movie review has a positive or negative sentiment.\n",
    "\n",
    "The data was collected by Stanford researchers and was used in a 2011 paper where a split of 50-50 of the data was used for training and test. **An accuracy of 88.89% was achieved.**\n",
    "\n",
    "### Keras advantages:\n",
    "As stated earlier, Keras was built with a focus on fast experimentation and prototyping. Hence,Keras provides access to the IMDB dataset built-in! \n",
    "\n",
    "The **imdb.load_data()** function allows you to load the dataset in a format that is ready for use in neural network and deep learning models.\n",
    "\n",
    "The words have been replaced by integers that indicate the ordered frequency of each word in the dataset. The sentences in each review are therefore comprised of a sequence of integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_review(raw_review, remove_symbol = True, remove_stopwords = False, output_format = \"string\"):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "            raw_review: raw text of a movie review\n",
    "            remove_stopwords: a boolean variable to indicate whether to remove stop words\n",
    "            output_format: if \"string\", return a cleaned string \n",
    "                           if \"list\", a list of words extracted from cleaned string.\n",
    "    Output:\n",
    "            Cleaned string or list.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Remove HTML markup\n",
    "    text = BeautifulSoup(raw_review)\n",
    "    \n",
    "    # Keep only characters\n",
    "    if remove_symbol:\n",
    "        text = re.sub(\"[^a-zA-Z]\", \" \", text.get_text())\n",
    "    \n",
    "    # Split words and store to list\n",
    "    text = text.lower().split()\n",
    "    \n",
    "    if remove_stopwords:\n",
    "    \n",
    "        # Use set as it has O(1) lookup time\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        words = [w for w in text if w not in stops]\n",
    "    \n",
    "    else:\n",
    "        words = text\n",
    "    \n",
    "    # Return a cleaned string or list\n",
    "    if output_format == \"string\":\n",
    "        return \" \".join(words)\n",
    "        \n",
    "    elif output_format == \"list\":\n",
    "        return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # fix random seed for reproducibility\n",
    "# np.random.seed(7)\n",
    "# # load the dataset but only keep the top n words, zero the rest\n",
    "# top_words = 5000\n",
    "# (X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"TrainSet.csv\", header=0, delimiter=\",\", quoting=0)\n",
    "# print(train_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python3.6/site-packages/bs4/__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 193 of the file /anaconda/lib/python3.6/runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP})\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP, \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning training review 0\n",
      "Cleaning training review 1000\n",
      "Cleaning training review 2000\n",
      "Cleaning training review 3000\n",
      "Cleaning training review 4000\n",
      "Cleaning test review 0\n",
      "Cleaning test review 1000\n"
     ]
    }
   ],
   "source": [
    "train_list = []\n",
    "test_list = []\n",
    "train_rate = []\n",
    "test_rate = []\n",
    "word2vec_input = []\n",
    "pred = []\n",
    "\n",
    "train_data = pd.read_csv(\"TrainSet1.csv\", header=0, delimiter=\",\", quoting=0)\n",
    "test_data = pd.read_csv(\"TestSet1.csv\", header=0, delimiter=\",\", quoting=0)\n",
    "\n",
    "# vector_type = \"Word2vec\"\n",
    "# Extract words from reviews\n",
    "# xrange is faster when iterating\n",
    "for i in range(0, len(train_data.review)):\n",
    "\n",
    "    # Append raw texts rather than lists as Count/TFIDF vectorizers take raw texts as inputs\n",
    "    train_list.append(clean_review(train_data.review[i]))\n",
    "    train_rate.append(train_data.rating[i])\n",
    "    if i%1000 == 0:\n",
    "        print (\"Cleaning training review\", i)\n",
    "\n",
    "for i in range(0, len(test_data.review)):\n",
    "\n",
    "    # Append raw texts rather than lists as Count/TFIDF vectorizers take raw texts as inputs\n",
    "    test_list.append(clean_review(test_data.review[i]))\n",
    "    test_rate.append(test_data.rating[i])\n",
    "    if i%1000 == 0:\n",
    "        print (\"Cleaning test review\", i)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "top_words = 5000\n",
    "unigram = []\n",
    "for i in range(len(train_list)):\n",
    "    unigram += train_list[i].split()\n",
    "unigram = Counter(unigram)\n",
    "\n",
    "# print(unigram)\n",
    "unigram = sorted(unigram.items(),key = lambda x:x[1],reverse = True)\n",
    "\n",
    "rank5000 = {k:i for i,(k,v) in enumerate(unigram) if i < top_words}\n",
    "rank = {k:i for i,(k,v) in enumerate(unigram)}\n",
    "# print(rank[defencei])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "okay admit it sometimes movies are so awful so over the top ridiculous that they re good and a lot of fun to laugh at swordfish directed by dominic sena gone in seconds and starring john travolta is one such film and unlike travolta s magnum opus of badness battlefield earth swordfish has so few pretensions that it invites you to laugh along with the script rather like the bouncing ball of music sing alongs since the film s buzz has stemmed mainly from halle berry s infamous naked breast scene perhaps i should start with the warning that the scene is about as exciting as a naked bust in an art museum during his soliloquy the camera goes in and out of focus so many times that your head will feel like it may explode speaking of explosions things that go boom in swordfish don t just ignite they go close to thermonuclear swordfish is the sort of movie that relishes its glorification of stupidity the script by skip woods is quite educational we learn that bit encryption is pretty secure stuff unless someone with stan s skills who kind of intuits his ways through the decoding is trying to break it but bit is really tough stuff are you taking notes you ve probably always wondered exactly how superstar programmers work well swordfish shows that to do world class hacking you need nine monitors in a movie this silly it s hard to pick a favorite moment for me it occurs after gabriel stands up in his fast expensive sports convertible to simultaneously shoot down sleek black sport utes on both sides others however may prefer the film s way way over the top finale actually the more i think about it the story s ludicrous big secret may be the biggest hoot of all it is rated r for violence language and some sexuality nudity and would be acceptable for most teenagers in the silicon valley it will be showing at the amc and the century theaters just send me a letter with the word subscribe in the subject line\n"
     ]
    }
   ],
   "source": [
    "print(train_list[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Each word is represented by Tag if it is a top 5000 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sent2tags(sentence):\n",
    "    \"\"\" Returns a vector of tag-classes from a given\n",
    "        sentence.\n",
    "    \"\"\"\n",
    "    tags = word_tokenize(sentence)\n",
    "    tags = nltk.pos_tag(tags)\n",
    "    out = []\n",
    "\n",
    "    for _, tag in tags[:500]:\n",
    "        if tag in postags:\n",
    "            out.append(t2k.get(tag))\n",
    "        else:\n",
    "            out.append(t2k.get(\"UKN\"))\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36\n"
     ]
    }
   ],
   "source": [
    "postags = [\"CC\", \"CD\", \"DT\", \"EX\", \"FW\", \"IN\", \"JJ\", \"JJR\", \"JJS\",\n",
    "           \"LS\", \"MD\", \"NN\", \"NNS\", \"NNP\", \"NNPS\", \"PDT\", \"POS\", \"PRP\",\n",
    "           \"PRP$\", \"RB\", \"RBR\", \"RBS\", \"RP\", \"TO\", \"UH\", \"VB\", \"VBD\", \"VBG\",\n",
    "           \"VBN\", \"VBP\", \"VBZ\", \"WDT\", \"WP\", \"WP$\", \"WRB\", \"UKN\"]\n",
    "nb_postags = len(postags)\n",
    "\n",
    "t2k = dict([(v,k) for k, v in enumerate(postags)])\n",
    "k2t = dict([(k,v) for k, v in enumerate(postags)])\n",
    "\n",
    "print(nb_postags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word2tag = {}\n",
    "# for i,(k,v) in enumerate(rank5000.items()):\n",
    "for i,(k,v) in enumerate(rank.items()):\n",
    "#     print(k)\n",
    "    tags = nltk.pos_tag([k])\n",
    "#     print(i,tags)\n",
    "    for _, tag in tags:\n",
    "        if tag in postags:\n",
    "            word2tag[k] = t2k.get(tag)\n",
    "        else:\n",
    "            word2tag[k] = t2k.get(\"UKN\")\n",
    "# print(word2tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Each word is represented by its index in the Term Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1108, 1641, 6, 356, 150, 18, 42, 1013, 42, 135, 0, 209, 700, 9, 44, 140, 72, 3, 2, 216, 1, 254, 4, 528, 34, 413, 27, 969, 8, 2382, 3, 717, 230, 2462, 5, 22, 102, 12, 3, 849, 2462, 7, 1, 1267, 33, 42, 92, 9, 6, 21, 4, 528, 397, 13, 0, 97, 236, 37, 0, 3172, 1, 180, 3027, 167, 0, 12, 7, 33, 1034, 41, 7, 1619, 134, 243, 16, 142, 626, 13, 0, 2383, 9, 0, 134, 5, 36, 11, 1569, 11, 2, 1619, 8, 25, 453, 313, 28, 0, 437, 386, 8, 3, 60, 1, 773, 42, 82, 234, 9, 93, 629, 62, 187, 37, 6, 122, 1672, 1, 1939, 221, 9, 210, 8, 85, 19, 59, 44, 210, 438, 4, 5, 0, 367, 1, 23, 9, 38, 1, 2706, 0, 97, 27, 2756, 2707, 5, 183, 75, 1039, 9, 264, 5, 454, 757, 1344, 288, 13, 7, 1877, 49, 118, 1, 28, 638, 147, 0, 5, 345, 4, 1313, 6, 15, 264, 5, 165, 1261, 757, 18, 21, 507, 1940, 21, 270, 206, 235, 4766, 430, 78, 139, 96, 357, 9, 4, 80, 226, 910, 21, 358, 1722, 8, 2, 23, 14, 602, 6, 7, 178, 4, 2193, 2, 703, 393, 10, 91, 6, 2158, 170, 4382, 1673, 55, 8, 28, 568, 3614, 1570, 4, 4280, 1354, 242, 352, 3446, 24, 149, 1723, 459, 171, 122, 2414, 0, 12, 7, 95, 95, 135, 0, 209, 1808, 251, 0, 32, 16, 156, 36, 6, 0, 43, 7, 2463, 169, 1594, 122, 17, 0, 1180, 1, 39, 6, 5, 120, 179, 10, 151, 256, 3, 52, 850, 269, 3, 50, 17, 362, 10, 48, 241, 8, 0, 4281, 2464, 6, 62, 17, 569, 34, 0, 4383, 3, 0, 964, 1209, 59, 944, 91, 2, 957, 13, 0, 468, 332, 8, 0, 201, 211]\n",
      "309\n",
      "[11, 11, 17, 19, 12, 29, 19, 11, 19, 5, 2, 11, 6, 5, 17, 11, 6, 0, 2, 11, 5, 11, 23, 11, 5, 11, 28, 5, 11, 11, 28, 5, 12, 0, 27, 11, 11, 30, 1, 6, 11, 0, 5, 11, 11, 11, 11, 5, 11, 11, 11, 11, 30, 19, 6, 12, 5, 17, 12, 17, 23, 11, 5, 5, 2, 11, 19, 5, 2, 27, 11, 5, 11, 27, 12, 5, 2, 11, 11, 11, 30, 26, 19, 5, 11, 11, 11, 6, 6, 11, 11, 19, 11, 10, 11, 5, 2, 27, 5, 2, 11, 30, 5, 5, 27, 5, 2, 6, 11, 5, 2, 11, 11, 5, 18, 11, 2, 11, 30, 5, 0, 5, 5, 11, 19, 6, 12, 5, 18, 11, 10, 11, 5, 17, 10, 11, 27, 5, 12, 12, 5, 25, 11, 5, 11, 11, 11, 19, 11, 17, 25, 19, 23, 11, 11, 30, 2, 11, 5, 11, 5, 12, 18, 11, 5, 11, 2, 11, 5, 11, 12, 30, 19, 6, 17, 11, 5, 11, 11, 30, 19, 11, 11, 5, 11, 5, 11, 11, 12, 32, 11, 5, 12, 18, 12, 5, 2, 27, 30, 27, 23, 11, 17, 0, 11, 30, 19, 6, 11, 29, 17, 27, 12, 17, 11, 19, 19, 26, 19, 34, 11, 12, 11, 19, 11, 12, 5, 23, 25, 11, 11, 27, 17, 11, 1, 12, 5, 2, 11, 2, 19, 17, 11, 6, 23, 11, 2, 11, 11, 5, 17, 17, 12, 5, 11, 12, 19, 5, 18, 11, 6, 12, 6, 23, 19, 11, 19, 11, 6, 11, 12, 5, 2, 12, 12, 19, 10, 11, 2, 11, 11, 11, 11, 5, 2, 11, 11, 19, 2, 20, 11, 11, 5, 17, 2, 11, 11, 6, 6, 11, 10, 25, 2, 8, 11, 5, 2, 17, 30, 28, 11, 5, 11, 11, 0, 2, 11, 11, 0, 10, 25, 6, 5, 8, 12, 5, 2, 11, 11, 17, 10, 25, 27, 5, 2, 11, 0, 2, 11, 12, 19, 11, 17, 2, 11, 5, 2, 11, 11, 5, 2, 11, 11]\n",
      "353\n"
     ]
    }
   ],
   "source": [
    "temp_train_list = copy.deepcopy(train_list)\n",
    "tag_train_list = [[] for i in range(len(train_list))]\n",
    "for i in range(len(temp_train_list)):\n",
    "    temp_train_list[i] = temp_train_list[i].split()\n",
    "    for j in range(len(temp_train_list[i])):\n",
    "        try:\n",
    "            temp = temp_train_list[i][j]\n",
    "            temp_train_list[i][j] = rank5000[temp_train_list[i][j]]\n",
    "            tag_train_list[i].append(word2tag[temp])\n",
    "        except:\n",
    "            temp = temp = temp_train_list[i][j]\n",
    "            temp_train_list[i][j] = 5001\n",
    "            tag_train_list[i].append(word2tag[temp])\n",
    "    temp_train_list[i] = list(filter(lambda x:x<=5000,temp_train_list[i]))\n",
    "print(temp_train_list[0])\n",
    "print(len(temp_train_list[0]))\n",
    "print(tag_train_list[0])\n",
    "print(len(tag_train_list[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[42, 47, 50, 1346, 2486, 520, 3, 36, 1236, 2486, 340, 4, 4305, 4849, 21, 4305, 391, 91, 446, 21, 55, 1559, 9, 0, 200, 1380, 1191, 260, 81, 149, 1380, 1191, 67, 18, 3, 222, 66, 104, 1, 0, 780, 3, 1251, 4712, 1, 1191, 496, 0, 2863, 10, 28, 150, 18, 697, 42, 954, 9, 137, 1380, 1191, 6, 50, 17, 152, 700, 4, 185, 4, 68, 81, 4585, 2496, 617, 8, 33, 59, 74, 140, 617, 137, 1900, 0, 12, 4, 17, 8, 2435, 4, 12, 883, 14, 5, 206, 934, 2009, 167, 619, 0, 1380, 1191, 67, 14, 22, 5, 206, 22, 1, 0, 3, 46, 21, 26, 20, 186, 28, 150, 213, 21, 130, 17, 4288, 4, 4251, 93, 629, 3, 1352, 60, 313, 0, 103, 164, 1, 14, 22, 11, 6, 7, 183, 2463, 1220, 50, 17, 2, 1150, 128, 185, 8, 0, 103, 272, 5, 143, 3115, 382, 21, 140, 29, 1624, 207, 104, 1380, 286, 8, 0, 12, 7, 1367, 1808, 266, 4, 4478, 22, 1192, 3546, 170, 184, 0, 416, 23, 5, 77, 32, 40, 2, 10, 1380, 4, 64, 141, 28, 3546, 1877, 8, 2, 1125, 866, 8, 14, 1511, 803, 2624, 20, 20, 20, 15, 340, 34, 2, 58, 3, 1083, 3405, 59, 213, 31, 148, 363, 4, 17, 4396, 406, 4, 2, 1397, 128, 28, 880, 67, 18, 53, 126, 14, 22, 99, 26, 38, 217, 0, 133, 102, 11, 6, 5, 2583, 0, 1, 688, 8, 3439, 2560, 11, 31, 5, 10, 3, 4425, 65, 22, 42, 99, 0, 87, 0, 23, 5, 95, 135, 0, 209, 1179, 15, 10, 1380, 7, 734, 178, 711, 56, 23, 1, 28, 51, 17, 70, 4030, 10, 48, 1, 141, 75, 380, 17, 126, 160, 22, 1, 28, 880, 32, 2975, 150, 3, 2494, 24, 14, 140, 747, 6, 5, 120, 202, 10, 1125, 151, 320, 450, 3, 271, 3, 50, 17, 176, 10, 191, 35, 3, 55, 76, 460, 609, 316, 307, 0, 12, 1589, 3, 128, 31, 580, 0, 3546, 31, 588, 0, 23, 66, 28, 1338, 996, 316, 324, 29, 30, 2, 77, 70, 53, 2453, 15, 580, 0, 121, 109, 4, 159, 0, 23, 28, 4585, 230, 324, 0, 23, 30, 165, 72, 3, 265, 63, 1338, 1283, 152, 324, 0, 23, 30, 3, 30, 2, 232, 79]\n",
      "392\n",
      "[19, 32, 10, 11, 11, 12, 0, 5, 11, 11, 12, 23, 11, 8, 17, 11, 25, 17, 11, 17, 19, 11, 5, 2, 6, 11, 11, 12, 17, 2, 11, 11, 12, 29, 35, 0, 6, 19, 5, 5, 2, 11, 11, 0, 6, 11, 5, 11, 17, 2, 12, 5, 18, 12, 29, 19, 19, 11, 5, 5, 11, 11, 17, 10, 25, 19, 6, 23, 11, 27, 23, 25, 17, 11, 12, 19, 28, 5, 30, 19, 28, 11, 28, 19, 5, 27, 2, 11, 23, 25, 28, 5, 11, 23, 11, 12, 2, 30, 19, 11, 11, 5, 5, 2, 11, 11, 12, 2, 1, 30, 19, 1, 5, 2, 11, 0, 8, 5, 17, 25, 19, 28, 18, 12, 5, 17, 10, 25, 6, 23, 11, 18, 11, 0, 11, 5, 5, 2, 19, 12, 5, 2, 1, 5, 17, 11, 19, 6, 27, 10, 25, 2, 11, 5, 27, 5, 2, 19, 11, 30, 19, 6, 19, 17, 11, 19, 11, 5, 5, 11, 19, 5, 2, 11, 11, 6, 11, 30, 23, 11, 1, 27, 11, 12, 11, 5, 2, 2, 6, 11, 30, 6, 20, 5, 2, 11, 5, 11, 23, 11, 17, 18, 11, 12, 12, 5, 2, 11, 27, 5, 2, 6, 11, 11, 12, 19, 19, 19, 0, 12, 5, 2, 11, 0, 19, 12, 19, 5, 17, 30, 6, 23, 25, 11, 19, 23, 2, 11, 5, 18, 19, 12, 29, 6, 20, 2, 1, 30, 25, 18, 12, 2, 11, 6, 5, 17, 30, 12, 2, 12, 5, 11, 11, 5, 11, 11, 5, 17, 30, 11, 5, 11, 0, 11, 11, 34, 1, 12, 19, 30, 2, 6, 2, 11, 30, 11, 5, 2, 11, 11, 0, 5, 11, 11, 11, 6, 12, 2, 11, 5, 18, 10, 25, 19, 11, 5, 8, 5, 17, 17, 11, 25, 20, 5, 27, 1, 5, 18, 19, 20, 12, 12, 0, 11, 5, 2, 11, 11, 17, 30, 28, 11, 5, 11, 11, 6, 12, 0, 11, 0, 10, 25, 11, 5, 12, 0, 0, 19, 18, 11, 11, 11, 11, 2, 11, 27, 0, 5, 17, 28, 2, 11, 12, 17, 26, 2, 11, 19, 18, 11, 11, 11, 11, 19, 26, 2, 6, 19, 6, 27, 0, 28, 2, 11, 19, 23, 25, 2, 11, 18, 11, 11, 11, 2, 11, 26, 19, 6, 0, 11, 18, 11, 11, 19, 11, 2, 11, 26, 11, 0, 26, 2, 11, 11]\n",
      "414\n"
     ]
    }
   ],
   "source": [
    "temp_test_list = copy.deepcopy(test_list)\n",
    "tag_test_list = [[] for i in range(len(test_list))]\n",
    "for i in range(len(temp_test_list)):\n",
    "    temp_test_list[i] = temp_test_list[i].split()\n",
    "    for j in range(len(temp_test_list[i])):\n",
    "        try:\n",
    "            temp = temp_test_list[i][j]\n",
    "            temp_test_list[i][j] = rank5000[temp_test_list[i][j]]\n",
    "            tag_test_list[i].append(word2tag[temp])\n",
    "        except:\n",
    "            temp = temp_test_list[i][j]\n",
    "            temp_test_list[i][j] = 5001\n",
    "#             print(i,j)\n",
    "            try:\n",
    "                tag_test_list[i].append(word2tag[temp])\n",
    "            except:\n",
    "                tag_test_list[i].append(t2k.get(\"UKN\"))\n",
    "                \n",
    "    temp_test_list[i] = list(filter(lambda x:x<=5000,temp_test_list[i]))\n",
    "print(temp_test_list[0])\n",
    "print(len(temp_test_list[0]))\n",
    "print(tag_test_list[0])\n",
    "print(len(tag_test_list[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    }
   ],
   "source": [
    "print(rank5000[\"not\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embeddings\n",
    "\n",
    "The vocabulary of words in all the reviews is very large. Mere one-hot encoding of individual words will lead to an extremely sparse dataset.\n",
    "\n",
    "Hence we will use Word Embeddings, a technique where words are encoded as real-valued vectors in a high dimensional space, where the similarity between words in terms of meaning translates to closeness in the vector space. This reduces sparsity of the data, as well as gives more meaning to each word in this embedded space, rather than just present or not. For more information on word embeddings, check out my experiments with Word Embeddings on Harry Potter and Game of Thrones [here](https://github.com/darshanbagul/Word_Embeddings)!\n",
    "\n",
    "In this notebook we won't have to use Gensim or create an embedding network from scratch. This is another advantage of using Keras. We just include another layer after the input in our model for generating embeddings of the input word! Keras provides a convenient way to convert positive integer representations of words into a word embedding by an **Embedding layer.**\n",
    "\n",
    "We will map each word onto a **32 length real valued vector**. We will also limit the total number of words that we are interested in modeling to the **5000 most frequent words, and zero out the rest.** Finally, the sequence length (number of words) in each review varies, so we will **constrain each review to be 500 words,** truncating long reviews and pad the shorter reviews with zero values. (Another alternative for this could be experimenting with a Dynamic RNN, but that is something I shall experiment with later.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = temp_train_list\n",
    "X_test = temp_test_list\n",
    "\n",
    "tag_X_train = tag_train_list\n",
    "tag_X_test = tag_test_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# truncate and pad input sequences\n",
    "max_review_length = 500\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=max_review_length)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=max_review_length)\n",
    "\n",
    "tag_X_train = sequence.pad_sequences(tag_X_train, maxlen=max_review_length)\n",
    "tag_X_test = sequence.pad_sequences(tag_X_test, maxlen=max_review_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models\n",
    "\n",
    "Now that we have preprocessed the dataset and split into train and test, let us begin exploring with different model architectures. The models that we are going to implement are listed below with increasing complexity:\n",
    "\n",
    "CNN + LSTM with Dropout regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM and Convolutional Neural Network with dropout\n",
    "\n",
    "Convolutional neural networks (CNNs) generally excel at learning the spatial structure in input data. Hence they are widely used on data which comprise of highly correlated spatial structures. For example - Images! Images are unstructured data points, where groups of pixels represent a particular structure. Presence or absence of such structures, helps us classify images into particular category.\n",
    "\n",
    "The IMDB review data does have a one-dimensional spatial structure in the sequence of words in reviews and the CNN may be able to pick out invariant features for good and bad sentiment. This learned spatial features may then be learned as sequences by an LSTM layer.\n",
    "\n",
    "Adding a 1-D Convolutional layer followed by Max pooling is elementary in Keras. \n",
    "\n",
    "Here we will add a 1-D Conv layer and max pooling layer after the Embedding layer which then feed the consolidated features to the LSTM. We use a set of 32 feature maps (convolutional filters) with a size of 3x3. The pooling layer can use the standard length of 2 to halve the feature map size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # create the model\n",
    "# max_review_length = 500\n",
    "# embedding_vecor_length = 32\n",
    "# model = Sequential()\n",
    "# model.add(Embedding(top_words, embedding_vecor_length, input_length=max_review_length))\n",
    "# model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
    "# model.add(MaxPooling1D(pool_size=2))\n",
    "# model.add(Dropout(0.2))\n",
    "# model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "# model.add(Dense(1, activation='sigmoid'))\n",
    "# model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # create the model\n",
    "# # max_review_length = 500\n",
    "# MAX_LEN = max_review_length\n",
    "# embedding_vecor_length = 32\n",
    "# model = Sequential()\n",
    "# main_input = Input(shape=(MAX_LEN,), dtype='int32', name='main_input')\n",
    "# model.add(Embedding(top_words, embedding_vecor_length, input_length=max_review_length,trainable=True))\n",
    "# model.add(SpatialDropout1D(0.2))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
    "# model.add(MaxPooling1D(pool_size=5))\n",
    "# model.add(Dropout(0.2))\n",
    "# model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "# # model.add(Bidirectional(LSTM(100, dropout=0.2, recurrent_dropout=0.2)))\n",
    "# model.add(Dense(2, activation='sigmoid'))\n",
    "# model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MultiLossLayer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/chrisliu/.local/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py:497: calling conv1d (from tensorflow.python.ops.nn_ops) with data_format=NHWC is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "`NHWC` for data_format is deprecated, use `NWC` instead\n",
      "WARNING:tensorflow:From /anaconda/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:1188: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:25: UserWarning: The `merge` function is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "/anaconda/lib/python3.6/site-packages/keras/legacy/layers.py:458: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "  name=name)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /anaconda/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:1290: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "main_input (InputLayer)      (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 500, 32)           160000    \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_1 (Spatial (None, 500, 32)           0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 500, 32)           128       \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 500, 32)           3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 250, 32)           0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 250, 32)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 200)               106400    \n",
      "_________________________________________________________________\n",
      "sec_output_tag (Dense)       (None, 5)                 1005      \n",
      "=================================================================\n",
      "Total params: 270,637\n",
      "Trainable params: 270,573\n",
      "Non-trainable params: 64\n",
      "_________________________________________________________________\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "main_input (InputLayer)          (None, 500)           0                                            \n",
      "____________________________________________________________________________________________________\n",
      "sec_input_tag (InputLayer)       (None, 500)           0                                            \n",
      "____________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)          (None, 500, 32)       160000      main_input[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)          (None, 500, 32)       160000      sec_input_tag[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "spatial_dropout1d_1 (SpatialDrop (None, 500, 32)       0           embedding_1[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "spatial_dropout1d_2 (SpatialDrop (None, 500, 32)       0           embedding_2[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNorm (None, 500, 32)       128         spatial_dropout1d_1[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNorm (None, 500, 32)       128         spatial_dropout1d_2[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)                (None, 500, 32)       3104        batch_normalization_1[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)                (None, 500, 32)       3104        batch_normalization_2[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1D)   (None, 250, 32)       0           conv1d_1[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1D)   (None, 250, 32)       0           conv1d_2[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)              (None, 250, 32)       0           max_pooling1d_1[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)              (None, 250, 32)       0           max_pooling1d_2[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional)  (None, 200)           106400      dropout_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional)  (None, 200)           106400      dropout_2[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "merge_1 (Merge)                  (None, 400)           0           bidirectional_1[0][0]            \n",
      "                                                                   bidirectional_2[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "dense_1 (Dense)                  (None, 30)            12030       merge_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)              (None, 30)            0           dense_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "main_output (Dense)              (None, 5)             155         dropout_3[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 551,449\n",
      "Trainable params: 551,321\n",
      "Non-trainable params: 128\n",
      "____________________________________________________________________________________________________\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "main_input (InputLayer)          (None, 500)           0                                            \n",
      "____________________________________________________________________________________________________\n",
      "sec_input_rating (InputLayer)    (None, 500)           0                                            \n",
      "____________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)          (None, 500, 32)       160000      main_input[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "embedding_3 (Embedding)          (None, 500, 32)       160000      sec_input_rating[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "spatial_dropout1d_1 (SpatialDrop (None, 500, 32)       0           embedding_1[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "spatial_dropout1d_3 (SpatialDrop (None, 500, 32)       0           embedding_3[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNorm (None, 500, 32)       128         spatial_dropout1d_1[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNorm (None, 500, 32)       128         spatial_dropout1d_3[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)                (None, 500, 32)       3104        batch_normalization_1[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)                (None, 500, 32)       3104        batch_normalization_3[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1D)   (None, 250, 32)       0           conv1d_1[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1D)   (None, 250, 32)       0           conv1d_3[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)              (None, 250, 32)       0           max_pooling1d_1[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)              (None, 250, 32)       0           max_pooling1d_3[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional)  (None, 200)           106400      dropout_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "lstm_3 (LSTM)                    (None, 100)           53200       dropout_4[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "merge_2 (Merge)                  (None, 300)           0           bidirectional_1[0][0]            \n",
      "                                                                   lstm_3[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "sec_output_rating (Dense)        (None, 5)             1505        merge_2[0][0]                    \n",
      "====================================================================================================\n",
      "Total params: 487,569\n",
      "Trainable params: 487,441\n",
      "Non-trainable params: 128\n",
      "____________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:39: UserWarning: The `merge` function is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:48: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=[<tf.Tenso...)`\n",
      "/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:49: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=[<tf.Tenso...)`\n",
      "/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:50: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=[<tf.Tenso...)`\n"
     ]
    }
   ],
   "source": [
    "# create the model\n",
    "# max_review_length = 500\n",
    "MAX_LEN = max_review_length\n",
    "embedding_vecor_length = 32\n",
    "model = Sequential()\n",
    "main_input = Input(shape=(MAX_LEN,), dtype='int32', name='main_input')\n",
    "x = Embedding(top_words, embedding_vecor_length, input_length=max_review_length,trainable=True)(main_input)\n",
    "x = SpatialDropout1D(0.2)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv1D(filters=32, kernel_size=3, padding='same', activation='relu')(x)\n",
    "x = MaxPooling1D(pool_size=2)(x)\n",
    "x = Dropout(0.2)(x)\n",
    "x = Bidirectional(LSTM(100, dropout=0.2, recurrent_dropout=0.2))(x)\n",
    "\n",
    "\n",
    "sec_input_tag = Input(shape=(MAX_LEN,), dtype='float32', name='sec_input_tag')\n",
    "y = Embedding(top_words, embedding_vecor_length, input_length=max_review_length,trainable=True)(sec_input_tag)\n",
    "y = SpatialDropout1D(0.2)(y)\n",
    "y = BatchNormalization()(y)\n",
    "y = Conv1D(filters=32, kernel_size=3, padding='same', activation='relu')(y)\n",
    "y = MaxPooling1D(pool_size=2)(y)\n",
    "y = Dropout(0.2)(y)\n",
    "y = Bidirectional(LSTM(100, dropout=0.2, recurrent_dropout=0.2))(y)\n",
    "\n",
    "xy = merge([x, y], mode='concat')\n",
    "xy = Dense(30, activation='tanh', )(xy)\n",
    "xy = Dropout(0.5)(xy)\n",
    "\n",
    "\n",
    "sec_input_rate = Input(shape=(MAX_LEN,), dtype='float32', name='sec_input_rating')\n",
    "z = Embedding(top_words, embedding_vecor_length, input_length=max_review_length,trainable=True)(sec_input_rate)\n",
    "z = SpatialDropout1D(0.2)(z)\n",
    "z = BatchNormalization()(z)\n",
    "z = Conv1D(filters=32, kernel_size=3, padding='same', activation='relu')(z)\n",
    "z = MaxPooling1D(pool_size=2)(z)\n",
    "z = Dropout(0.2)(z)\n",
    "z = LSTM(100, dropout=0.2, recurrent_dropout=0.2)(z)\n",
    "\n",
    "xz = merge([x, z], mode='concat')\n",
    "# xz = Dense(30, activation='tanh', )(xz)\n",
    "# xz = Dropout(0.5)(xz)\n",
    "\n",
    "task0_output = Dense(5, activation='sigmoid', name='sec_output_tag')(x)\n",
    "task1_output = Dense(5, activation='softmax', name='main_output')(xy)\n",
    "task2_output = Dense(5, activation='softmax', name='sec_output_rating')(xz)\n",
    "\n",
    "\n",
    "model_task0 = Model(input=[main_input], output=[task0_output])\n",
    "model_task1 = Model(input=[main_input, sec_input_tag], output=[task1_output])\n",
    "model_task2 = Model(input=[main_input, sec_input_rate], output=[task2_output])\n",
    "\n",
    "model_task0.compile(optimizer='RMSprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model_task1.compile(optimizer='RMSprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model_task2.compile(optimizer='RMSprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model_task0.summary()\n",
    "model_task1.summary()\n",
    "model_task2.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # create the model\n",
    "# # max_review_length = 500\n",
    "# MAX_LEN = max_review_length\n",
    "# embedding_vecor_length = 32\n",
    "# model = Sequential()\n",
    "# main_input = Input(shape=(MAX_LEN,), dtype='int32', name='main_input')\n",
    "# x = Embedding(top_words, embedding_vecor_length, input_length=max_review_length,trainable=True)(main_input)\n",
    "# x = SpatialDropout1D(0.2)(x)\n",
    "# x = BatchNormalization()(x)\n",
    "\n",
    "# x = Conv1D(filters=32, kernel_size=3, padding='same', activation='relu')(x)\n",
    "# x = MaxPooling1D(pool_size=2)(x)\n",
    "# x = Dropout(0.2)(x)\n",
    "\n",
    "# x = Bidirectional(LSTM(100, dropout=0.2, recurrent_dropout=0.2))(x)\n",
    "# task1_output = Dense(5, activation='softmax', name='main_output')(x)\n",
    "# task2_output = Dense(2, activation='softmax', name='aux_output')(x)\n",
    "\n",
    "\n",
    "# model_task1 = Model(input=[main_input], output=[task1_output])\n",
    "# model_task2 = Model(input=[main_input], output=[task2_output])\n",
    "\n",
    "# model_task1.compile(optimizer='RMSprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "# model_task2.compile(optimizer='RMSprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# model_task1.summary()\n",
    "# model_task2.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### map the train and test score into 5 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4004\n",
      "{1, 2, 3, 4, 5}\n"
     ]
    }
   ],
   "source": [
    "temp_y_train = copy.deepcopy(train_rate)\n",
    "temp_y_train_binary = copy.deepcopy(train_rate)\n",
    "for i in range(len(temp_y_train)):\n",
    "    if 0 <= temp_y_train[i] <= 5:\n",
    "        temp_y_train_binary[i] = 0\n",
    "    else:\n",
    "        temp_y_train_binary[i] = 1\n",
    "    \n",
    "    if 0 <= temp_y_train[i] <= 0.2:\n",
    "        temp_y_train[i] = 1\n",
    "    elif 0.2 < temp_y_train[i] <= 0.4:\n",
    "        temp_y_train[i] = 2\n",
    "    elif 0.4 < temp_y_train[i] <= 0.6:\n",
    "        temp_y_train[i] = 3\n",
    "    elif 0.6 < temp_y_train[i] <= 0.8:\n",
    "        temp_y_train[i] = 4\n",
    "    elif 0.8 < temp_y_train[i] <= 1:\n",
    "        temp_y_train[i] = 5\n",
    "\n",
    "print(len(temp_y_train))\n",
    "print(set(temp_y_train))\n",
    "# print(temp_y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1002\n",
      "{1, 2, 3, 4, 5}\n"
     ]
    }
   ],
   "source": [
    "temp_y_test = copy.deepcopy(test_rate)\n",
    "temp_y_test_binary = copy.deepcopy(test_rate)\n",
    "for i in range(len(temp_y_test)):\n",
    "    if 0 <= temp_y_test[i] <= 5:\n",
    "        temp_y_test_binary[i] = 0\n",
    "    else:\n",
    "        temp_y_test_binary[i] = 1\n",
    "    \n",
    "    if 0 <= temp_y_test[i] <= 0.2:\n",
    "        temp_y_test[i] = 1\n",
    "    elif 0.2 < temp_y_test[i] <= 0.4:\n",
    "        temp_y_test[i] = 2\n",
    "    elif 0.4 < temp_y_test[i] <= 0.6:\n",
    "        temp_y_test[i] = 3\n",
    "    elif 0.6 < temp_y_test[i] <= 0.8:\n",
    "        temp_y_test[i] = 4\n",
    "    elif 0.8 < temp_y_test[i] <= 1:\n",
    "        temp_y_test[i] = 5\n",
    "\n",
    "print(len(temp_y_test))\n",
    "print(set(temp_y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2001\n"
     ]
    }
   ],
   "source": [
    "print(rank5000[\"neutral\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "temp_train_rating = copy.deepcopy(train_rate)\n",
    "train_rating = [[] for i in range(len(temp_train_rating))]\n",
    "for i in range(len(temp_train_rating)):\n",
    "#     if temp_y_train[i] == 1:\n",
    "#         train_rating[i] += [rank5000[\"very\"], rank5000[\"positive\"]]\n",
    "#     elif temp_y_train[i] == 2:\n",
    "#         train_rating[i] += [rank5000[\"positive\"]]\n",
    "#     elif temp_y_train[i] == 3:\n",
    "#         train_rating[i] += [rank5000[\"neutral\"]]#[rank5000[\"not\"],rank5000[\"positive\"],rank5000[\"not\"],rank5000[\"negative\"]]\n",
    "#     elif temp_y_train[i] == 4:\n",
    "#         train_rating[i] += [rank5000[\"negative\"]]\n",
    "#     elif temp_y_train[i] == 5:\n",
    "#         train_rating[i] += [rank5000[\"very\"], rank5000[\"negative\"]]\n",
    "\n",
    "    if temp_y_train[i] == 1:\n",
    "        train_rating[i] += [1]\n",
    "    elif temp_y_train[i] == 2:\n",
    "        train_rating[i] += [2]\n",
    "    elif temp_y_train[i] == 3:\n",
    "        train_rating[i] += [3]\n",
    "    elif temp_y_train[i] == 4:\n",
    "        train_rating[i] += [4]\n",
    "    elif temp_y_train[i] == 5:\n",
    "        train_rating[i] += [5]\n",
    "# print(train_rating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "temp_test_rating = copy.deepcopy(test_rate)\n",
    "test_rating = [[] for i in range(len(temp_test_rating))]\n",
    "for i in range(len(temp_test_rating)):\n",
    "#     if temp_y_train[i] == 1:\n",
    "#         test_rating[i] += [rank5000[\"very\"], rank5000[\"positive\"]]\n",
    "#     elif temp_y_train[i] == 2:\n",
    "#         test_rating[i] += [rank5000[\"positive\"]]\n",
    "#     elif temp_y_train[i] == 3:\n",
    "#         test_rating[i] += [rank5000[\"neutral\"]]#[rank5000[\"not\"],rank5000[\"positive\"],rank5000[\"not\"],rank5000[\"negative\"]]\n",
    "#     elif temp_y_train[i] == 4:\n",
    "#         test_rating[i] += [rank5000[\"negative\"]]\n",
    "#     elif temp_y_train[i] == 5:\n",
    "#         test_rating[i] += [rank5000[\"very\"], rank5000[\"negative\"]]\n",
    "        \n",
    "    if temp_y_train[i] == 1:\n",
    "        test_rating[i] += [1]\n",
    "    elif temp_y_train[i] == 2:\n",
    "        test_rating[i] += [2]\n",
    "    elif temp_y_train[i] == 3:\n",
    "        test_rating[i] += [3]\n",
    "    elif temp_y_train[i] == 4:\n",
    "        test_rating[i] += [4]\n",
    "    elif temp_y_train[i] == 5:\n",
    "        test_rating[i] += [5]\n",
    "# print(train_rating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 5.976119402985074, 1: 0.9789731051344743, 2: 0.5488690884167238, 3: 0.6071266110689917, 4: 2.9226277372262772}\n"
     ]
    }
   ],
   "source": [
    "from sklearn import utils\n",
    "class_weights = utils.compute_class_weight('balanced', np.unique(temp_y_train), temp_y_train)\n",
    "class_weights= {class_id:class_weight for class_id, class_weight in zip(list(range(5)), class_weights)}\n",
    "# class_weights[0] = 0\n",
    "# class_weights = {k:v for k,v in sorted(class_weights.items(),key=lambda x:x[0])}\n",
    "print(class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 1 0]\n",
      " [0 0 0 1 0]\n",
      " [0 0 0 1 0]\n",
      " ...\n",
      " [0 0 1 0 0]\n",
      " [0 0 0 1 0]\n",
      " [0 0 1 0 0]]\n",
      "[[0 0 1 0 0]\n",
      " [0 0 0 1 0]\n",
      " [0 0 0 0 1]\n",
      " ...\n",
      " [0 0 0 1 0]\n",
      " [0 1 0 0 0]\n",
      " [0 0 0 1 0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "mlb = MultiLabelBinarizer(classes=[1,2,3,4,5])\n",
    "mlb_binary = MultiLabelBinarizer(classes=[0,1])\n",
    "y_train = mlb.fit_transform([[y] for y in temp_y_train])\n",
    "# y_train_binary = mlb_binary.fit_transform([[y] for y in temp_y_train_binary])\n",
    "print(y_train)\n",
    "\n",
    "y_test = mlb.fit_transform([[y] for y in temp_y_test])\n",
    "# y_test_binary = mlb_binary.fit_transform([[y] for y in temp_y_test_binary])\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 4]\n"
     ]
    }
   ],
   "source": [
    "rate_X_train = sequence.pad_sequences(train_rating, maxlen=max_review_length)\n",
    "rate_X_test = sequence.pad_sequences(test_rating, maxlen=max_review_length)\n",
    "print(rate_X_train[0])\n",
    "# rate_Y_train = sequence.pad_sequences([[y] for y in temp_y_train], maxlen=max_review_length)\n",
    "# rate_Y_test = sequence.pad_sequences([[y] for y in temp_y_test], maxlen=max_review_length)\n",
    "# print(rate_Y_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Custom loss layer\n",
    "class CustomMultiLossLayer(Layer):\n",
    "    def __init__(self, nb_outputs=2, **kwargs):\n",
    "        self.nb_outputs = nb_outputs\n",
    "        self.is_placeholder = True\n",
    "        super(CustomMultiLossLayer, self).__init__(**kwargs)\n",
    "        \n",
    "    def build(self, input_shape=None):\n",
    "        # initialise log_vars\n",
    "        self.log_vars = []\n",
    "        for i in range(self.nb_outputs):\n",
    "            self.log_vars += [self.add_weight(name='log_var' + str(i), shape=(1,),\n",
    "                                              initializer=Constant(0.), trainable=True)]\n",
    "        super(CustomMultiLossLayer, self).build(input_shape)\n",
    "\n",
    "    def multi_loss(self, ys_true, ys_pred):\n",
    "#         print(self.nb_outputs)\n",
    "#         print(len(ys_true))\n",
    "#         print(len(ys_pred))\n",
    "        assert len(ys_true) == self.nb_outputs and len(ys_pred) == self.nb_outputs\n",
    "        loss = 0\n",
    "        for y_true, y_pred, log_var in zip(ys_true, ys_pred, self.log_vars):\n",
    "            precision = K.exp(-log_var[0])\n",
    "            loss += K.sum(precision * (y_true - y_pred)**2. + log_var[0], -1)\n",
    "        return K.mean(loss)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        ys_true = inputs[:self.nb_outputs]\n",
    "        ys_pred = inputs[self.nb_outputs:]\n",
    "        loss = self.multi_loss(ys_true, ys_pred)\n",
    "        self.add_loss(loss, inputs=inputs)\n",
    "        # We won't actually use the output.\n",
    "        return K.concatenate(inputs, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MAX_LEN = 500\n",
    "def get_prediction_model():\n",
    "    main_input = Input(shape=(MAX_LEN,), dtype='float32', name='main_input')\n",
    "    sec_input = Input(shape=(MAX_LEN,), dtype='float32', name='sec_input')\n",
    "    x = Embedding(top_words, embedding_vecor_length, input_length=max_review_length,trainable=True)(main_input)\n",
    "    x = SpatialDropout1D(0.2)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv1D(filters=32, kernel_size=3, padding='same', activation='relu')(x)\n",
    "    x = MaxPooling1D(pool_size=2)(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = Bidirectional(LSTM(100, dropout=0.2, recurrent_dropout=0.2))(x)\n",
    "    \n",
    "    y = Embedding(top_words, embedding_vecor_length, input_length=max_review_length,trainable=True)(sec_input)\n",
    "    y = SpatialDropout1D(0.2)(y)\n",
    "    y = BatchNormalization()(y)\n",
    "    y = Conv1D(filters=32, kernel_size=3, padding='same', activation='relu')(y)\n",
    "    y = MaxPooling1D(pool_size=2)(y)\n",
    "    y = Dropout(0.2)(y)\n",
    "    y = Bidirectional(LSTM(100, dropout=0.2, recurrent_dropout=0.2))(y)\n",
    "    \n",
    "    task1_output = Dense(5, activation='softmax', name='word_output')(x)\n",
    "    task2_output = Dense(5, activation='softmax', name='tag_output')(y)\n",
    "    return Model([main_input, sec_input], [task1_output, task2_output])\n",
    "\n",
    "def get_trainable_model(prediction_model):\n",
    "    main_input = Input(shape=(MAX_LEN,), dtype='float32', name='main_input')\n",
    "    sec_input = Input(shape=(MAX_LEN,), dtype='float32', name='sec_input')\n",
    "    task1_output, task2_output = prediction_model([main_input, sec_input])\n",
    "\n",
    "    task1_input = Input(shape=(5,), dtype='float32', name='task1_input')\n",
    "    task2_input = Input(shape=(5,), dtype='float32', name='task2_input')\n",
    "    out = CustomMultiLossLayer(nb_outputs=2)([task1_input, task2_input, task1_output, task2_output])\n",
    "    \n",
    "    xz = merge([task1_output, task1_output], mode='concat')\n",
    "    task_output = Dense(5, activation='softmax', name='sec_output_rating')(xz)\n",
    "    \n",
    "    return Model([main_input, sec_input, task1_input, task2_input], output = out)\n",
    "\n",
    "prediction_model = get_prediction_model()\n",
    "trainable_model = get_trainable_model(prediction_model)\n",
    "trainable_model.compile(optimizer='RMSprop', loss=None)\n",
    "trainable_model.compile(optimizer='RMSprop', loss=None, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hist = trainable_model.fit([X_train, tag_X_train, y_train, y_train], epochs=2, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# print(hist)\n",
    "scores = trainable_model.evaluate([X_test, tag_X_test,y_test,y_test], None, verbose=1)\n",
    "# print(\"Epoch: %d, Accuracy: %.2f%%\" % (i, scores[1]*100))\n",
    "# print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pylab\n",
    "%matplotlib inline\n",
    "pylab.plot(hist.history['loss'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model 3 for tag only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "results = []\n",
    "for batch in range(60):\n",
    "    t=time.time()\n",
    "    if random.random() < 0.8:\n",
    "        sample = np.random.randint(0, len(y_train), BATCH_SIZE)\n",
    "        x_sampled, y_sampled = tag_X_train[sample], y_train[sample]\n",
    "        model_task1.train_on_batch(x_sampled, y_sampled)#, class_weight=class_weights,sample_weight=None)\n",
    "    print(batch,time.time()-t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scores = model_task1.fit(tag_X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model 1 for sentence\n",
    "#### task1: top 5000 words\n",
    "#### task2: tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# BATCH_SIZE = 128\n",
    "# results = []\n",
    "# for batch in range(60):\n",
    "#     t=time.time()\n",
    "#     if random.random() < 0.8:\n",
    "#         sample = np.random.randint(0, len(y_train), BATCH_SIZE)\n",
    "#         x_sampled, y_sampled = tag_X_train[sample], y_train[sample]\n",
    "#         model_task1.train_on_batch(x_sampled, y_sampled)#, class_weight=class_weights,sample_weight=None)\n",
    "#     print(batch,time.time()-t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "4004/4004 [==============================] - 72s - loss: 1.4264 - acc: 0.3417    \n",
      "Epoch 2/10\n",
      "4004/4004 [==============================] - 67s - loss: 1.3846 - acc: 0.3624    \n",
      "Epoch 3/10\n",
      "4004/4004 [==============================] - 68s - loss: 1.3300 - acc: 0.4053    \n",
      "Epoch 4/10\n",
      "4004/4004 [==============================] - 71s - loss: 1.2646 - acc: 0.4548    \n",
      "Epoch 5/10\n",
      "4004/4004 [==============================] - 69s - loss: 1.1931 - acc: 0.4935    \n",
      "Epoch 6/10\n",
      "4004/4004 [==============================] - 72s - loss: 1.1199 - acc: 0.5310    \n",
      "Epoch 7/10\n",
      " 768/4004 [====>.........................] - ETA: 58s - loss: 1.0611 - acc: 0.5729"
     ]
    }
   ],
   "source": [
    "model_task1.fit([X_train, tag_X_train], y_train, epochs=10, batch_size=64)\n",
    "scores = model_task1.evaluate([X_test, tag_X_test], y_test, verbose=1)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model 2 for sentence\n",
    "#### task1: top 5000 words\n",
    "#### task2: rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_task2.fit([X_train, rate_X_train], y_train, epochs=10, batch_size=64)\n",
    "scores = model_task2.evaluate([X_test, rate_X_test], y_test, verbose=1)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model 0\n",
    "#### single task on word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_task0.fit(X_train, y_train, epochs=10, batch_size=64)\n",
    "scores = model_task0.evaluate(X_test, y_test, verbose=1)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# BATCH_SIZE = 128\n",
    "# results = []\n",
    "# for batch in range(60):\n",
    "#     t=time.time()\n",
    "#     if random.random() < 0.8:\n",
    "#         sample = np.random.randint(0, len(y_train), BATCH_SIZE)\n",
    "#         x_sampled, y_sampled = X_train[sample], y_train[sample]\n",
    "#         model_task1.train_on_batch(x_sampled, y_sampled)#, class_weight=class_weights,sample_weight=None)\n",
    "#     print(batch,time.time()-t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# scores = model_task1.evaluate(X_test, y_test, verbose=0)\n",
    "# print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### model2 for sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# BATCH_SIZE = 128\n",
    "# results = []\n",
    "# for batch in range(60):\n",
    "#     t=time.time()\n",
    "#     if random.random() < 0.8:\n",
    "#         sample = np.random.randint(0, len(y_train), BATCH_SIZE)\n",
    "#         x_sampled, y_sampled = X_train[sample], y_train_binary[sample]\n",
    "#         model_task2.train_on_batch(x_sampled, y_sampled)#, class_weight=class_weights,sample_weight=None)\n",
    "#     print(batch,time.time()-t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# scores = model_task2.evaluate(X_test, y_test_binary, verbose=0)\n",
    "# print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model 0 for sentence binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train_binary, epochs=4, batch_size=64)\n",
    "\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test_binary, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train, epochs=1, batch_size=64)\n",
    "\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "As we can see, with a mixture of a **Convolutional Neural network + LSTM**, we are able to achieve an accuracy as high as the state-of-art accuracy for this dataset. As stated earlier, the Stanford researchers were able to achieve an accuracy of 88.89%, and **we have been able to reach 88.18%** (I even achieved **88.41%** with a different seed earlier!)\n",
    "\n",
    "Some reflections:\n",
    "    1. CNN and max pooling layers after the Embedding layer are able to pick out invariant features for good and bad sentiment. These learned spatial features are then learned as sequences by an LSTM layer.\n",
    "    2. We have less weights and faster training time!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook we implemented LSTM network models for sequence classification problem, specifically sentiment classification of movie reviews from **Large Movie Review Dataset dataset.**\n",
    "\n",
    "    1. Implemented a simple single layer LSTM model for the IMDB movie review sentiment classification problem.\n",
    "    2. Extended LSTM model with LSTM-specific dropout to combat overfitting.\n",
    "    3. Combined the spatial structure learning properties of a Convolutional Neural Network (CNN) with the sequence learning of an LSTM."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
